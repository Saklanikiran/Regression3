{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36b95fd-3685-4e53-aa75-ba0bf1bee0d7",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c799e9f-d449-4f15-9342-a1ffa70daded",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ridge Regression is a technique used in regression analysis to mitigate the problem of multicollinearity (high correlation between predictor \n",
    "variables) and overfitting. It adds a penalty term to the ordinary least squares (OLS) regression method, which minimizes the sum of squared residuals.\n",
    "This penalty term is proportional to the square of the magnitude of coefficients, forcing them to be smaller. As a result, Ridge Regression shrinks\n",
    "the coefficients towards zero, reducing their variance and making the model less sensitive to the input data. Unlike ordinary least squares, which can \n",
    "lead to large coefficients, especially when dealing with correlated predictors, Ridge Regression tends to provide more stable and reliable estimates,\n",
    "even in the presence of multicollinearity.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0c541-9063-49c7-87a7-42572e6b24e6",
   "metadata": {},
   "source": [
    "# Ans : 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad718c-4f5e-4be3-ab02-a9c472cd5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ridge Regression shares many assumptions with ordinary least squares (OLS) regression, including linearity, independence of errors, constant variance of errors (homoscedasticity), and normality of errors. However, Ridge Regression also assumes that the predictor variables are not perfectly multicollinear, meaning they are not perfectly\n",
    "correlated with each other. While multicollinearity does not violate the assumptions of OLS regression, it can lead to inflated standard errors and \n",
    "unstable coefficient estimates. Ridge Regression addresses this by penalizing the coefficients, effectively reducing their variance and stabilizing\n",
    "the model. Additionally, Ridge Regression assumes that the penalty parameter, usually denoted as lambda (Î»), is appropriately chosen to balance the\n",
    "trade-off between bias and variance in the model, typically through techniques like cross-validation.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a8eeb-a858-451e-98bc-711401004f86",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724034e0-2417-4302-b1a4-e66498259a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Selecting the value of the tuning parameter (lambda) in Ridge Regression involves balancing the trade-off between bias and variance in the model.\n",
    "One common approach is cross-validation, where the dataset is divided into training and validation sets multiple times. For each division, the model \n",
    "is trained on the training set and evaluated on the validation set using different values of lambda. The value of lambda that results in the best\n",
    "performance metric (e.g., lowest mean squared error or highest R-squared) on the validation set is chosen. Another method is to use techniques \n",
    "like grid search or randomized search, where a predefined range of lambda values is specified, and the optimal value is selected based on its \n",
    "performance on a separate validation set or through nested cross-validation.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa21711-2b0c-459d-bf67-4b8c24aafd86",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb7ddd-abe2-448b-a1ec-56f63e4ff9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Yes, Ridge Regression can be used for feature selection to some extent. While its primary purpose is regularization to reduce overfitting, it\n",
    "indirectly performs feature selection by shrinking the coefficients of less important features towards zero. Features with coefficients close to \n",
    "zero are effectively removed from the model, as they contribute less to the predictions. By tuning the regularization parameter (lambda), Ridge \n",
    "Regression can control the degree of shrinkage, thereby influencing which features are retained or discarded. However, Ridge Regression does not\n",
    "perform explicit feature selection like some other methods (e.g., Lasso Regression), which directly force coefficients to be exactly zero. \n",
    "Nonetheless, by examining the magnitude of the coefficients after Ridge Regression, one can infer the importance of features in the model.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f68ed-cd5a-421c-8027-3eee3685dedb",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675e1f5-09a1-4ac6-aab8-7c8e6d8588c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ridge Regression performs well in the presence of multicollinearity, a situation where predictor variables are highly correlated. Unlike ordinary \n",
    "least squares regression, which can lead to unstable and inflated coefficient estimates in multicollinear scenarios, Ridge Regression mitigates this\n",
    "issue by shrinking the coefficients towards zero. By penalizing the magnitude of coefficients, Ridge Regression reduces their sensitivity to \n",
    "multicollinearity, resulting in more stable and reliable estimates. While it doesn't completely eliminate multicollinearity, it effectively \n",
    "reduces its impact on the model's performance. However, Ridge Regression doesn't provide explicit variable selection, so multicollinear predictors may\n",
    "still have non-zero coefficients, but they are attenuated relative to OLS estimates. Overall, Ridge Regression is a robust choice for handling \n",
    "multicollinearity in regression analysis.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a1b47-f7a8-4485-a46b-bdf72825ee0d",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78c82c-ebd4-4515-acc3-5494f78cc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables. However, categorical variables need to be converted into \n",
    "numerical form through techniques like one-hot encoding before being used in the regression model. One-hot encoding converts categorical variables \n",
    "into binary vectors, with each category represented by a binary variable (0 or 1). These binary variables are then treated as numerical predictors \n",
    "in the Ridge Regression model. Continuous variables remain unchanged and are directly included in the regression alongside the encoded categorical \n",
    "variables. Ridge Regression does not differentiate between the types of predictors; it treats all predictors, whether continuous or categorical\n",
    "(after encoding), in the same manner during model estimation and regularization.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1d84e-1fed-4a7c-abff-6eb39f078622",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e5919-142e-4e6b-9ec9-f4ad7610ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b501b6b-ea0c-4c72-a757-36115d93da6d",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d5a26-9c25-43c2-bcbf-14ffff90378c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
